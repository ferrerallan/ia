{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How TensorFlow works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 7]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "my_graph = tf.Graph()\n",
    "with tf.Session(graph=my_graph) as sess:\n",
    "    x = tf.constant([1,3,6]) \n",
    "    y = tf.constant([1,1,1])\n",
    "    op = tf.add(x,y)\n",
    "    result = sess.run(fetches=op)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to manipulate data and pass it to the Neural Network inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "text = \"Hi from Brazil\"\n",
    "\n",
    "for word in text.split(' '):\n",
    "    word_lowercase = word.lower()\n",
    "    vocab[word_lowercase]+=1\n",
    "        \n",
    "def get_word_2_index(vocab):\n",
    "    word2index = {}\n",
    "    for i,word in enumerate(vocab):\n",
    "        word2index[word] = i\n",
    "        \n",
    "    return word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi from Brazil: [ 1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "word2index = get_word_2_index(vocab)\n",
    "\n",
    "total_words = len(vocab)\n",
    "matrix = np.zeros((total_words),dtype=float)\n",
    "\n",
    "for word in text.split():\n",
    "    matrix[word2index[word.lower()]] += 1\n",
    "    \n",
    "print(\"Hi from Brazil:\", matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi: [ 1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "matrix = np.zeros((total_words),dtype=float)\n",
    "text = \"Hi\"\n",
    "for word in text.split():\n",
    "    matrix[word2index[word.lower()]] += 1\n",
    "    \n",
    "print(\"Hi:\", matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = [\"comp.graphics\",\"sci.space\",\"rec.sport.baseball\"]\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total texts in train: 1774\n",
      "total texts in test: 1180\n"
     ]
    }
   ],
   "source": [
    "print('total texts in train:',len(newsgroups_train.data))\n",
    "print('total texts in test:',len(newsgroups_test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text From: jk87377@lehtori.cc.tut.fi (Kouhia Juhana)\n",
      "Subject: Re: More gray levels out of the screen\n",
      "Organization: Tampere University of Technology\n",
      "Lines: 21\n",
      "Distribution: inet\n",
      "NNTP-Posting-Host: cc.tut.fi\n",
      "\n",
      "In article <1993Apr6.011605.909@cis.uab.edu> sloan@cis.uab.edu\n",
      "(Kenneth Sloan) writes:\n",
      ">\n",
      ">Why didn't you create 8 grey-level images, and display them for\n",
      ">1,2,4,8,16,32,64,128... time slices?\n",
      "\n",
      "By '8 grey level images' you mean 8 items of 1bit images?\n",
      "It does work(!), but it doesn't work if you have more than 1bit\n",
      "in your screen and if the screen intensity is non-linear.\n",
      "\n",
      "With 2 bit per pixel; there could be 1*c_1 + 4*c_2 timing,\n",
      "this gives 16 levels, but they are linear if screen intensity is\n",
      "linear.\n",
      "With 1*c_1 + 2*c_2 it works, but we have to find the best\n",
      "compinations -- there's 10 levels, but 16 choises; best 10 must be\n",
      "chosen. Different compinations for the same level, varies a bit, but\n",
      "the levels keeps their order.\n",
      "\n",
      "Readers should verify what I wrote... :-)\n",
      "\n",
      "Juhana Kouhia\n",
      "\n",
      "category: 0\n"
     ]
    }
   ],
   "source": [
    "print('text',newsgroups_train.data[0])\n",
    "print('category:',newsgroups_train.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "for text in newsgroups_train.data:\n",
    "    for word in text.split(' '):\n",
    "        vocab[word.lower()]+=1\n",
    "        \n",
    "for text in newsgroups_test.data:\n",
    "    for word in text.split(' '):\n",
    "        vocab[word.lower()]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 119930\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words:\",len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'the': 10\n"
     ]
    }
   ],
   "source": [
    "total_words = len(vocab)\n",
    "\n",
    "def get_word_2_index(vocab):\n",
    "    word2index = {}\n",
    "    for i,word in enumerate(vocab):\n",
    "        word2index[word.lower()] = i\n",
    "        \n",
    "    return word2index\n",
    "\n",
    "word2index = get_word_2_index(vocab)\n",
    "\n",
    "print(\"Index of the word 'the':\",word2index['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    layer = np.zeros(total_words,dtype=float)\n",
    "    for word in text.split(' '):\n",
    "        layer[word2index[word.lower()]] += 1\n",
    "        \n",
    "    return layer\n",
    "\n",
    "def category_to_vector(category):\n",
    "    y = np.zeros((3),dtype=float)\n",
    "    if category == 0:\n",
    "        y[0] = 1.\n",
    "    elif category == 1:\n",
    "        y[1] = 1.\n",
    "    else:\n",
    "        y[2] = 1.\n",
    "        \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(df,i,batch_size):\n",
    "    batches = []\n",
    "    results = []\n",
    "    texts = df.data[i*batch_size:i*batch_size+batch_size]\n",
    "    categories = df.target[i*batch_size:i*batch_size+batch_size]\n",
    "    \n",
    "    for text in texts:\n",
    "        layer = text_to_vector(text) \n",
    "        batches.append(layer)\n",
    "        \n",
    "    for category in categories:\n",
    "        y = category_to_vector(category)\n",
    "        results.append(y)  \n",
    "     \n",
    "    return np.array(batches),np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each batch has 100 texts and each matrix has 119930 elements (words): (100, 119930)\n"
     ]
    }
   ],
   "source": [
    "print(\"Each batch has 100 texts and each matrix has 119930 elements (words):\",get_batch(newsgroups_train,1,100)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each batch has 100 labels and each matrix has 3 elements (3 categories): (100, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Each batch has 100 labels and each matrix has 3 elements (3 categories):\",get_batch(newsgroups_train,1,100)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 10\n",
    "batch_size = 150\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 100      # 1st layer number of features\n",
    "n_hidden_2 = 100       # 2nd layer number of features\n",
    "n_input = total_words # Words in vocab\n",
    "n_classes = 3         # Categories: graphics, sci.space and baseball\n",
    "\n",
    "input_tensor = tf.placeholder(tf.float32,[None, n_input],name=\"input\")\n",
    "output_tensor = tf.placeholder(tf.float32,[None, n_classes],name=\"output\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(input_tensor, weights, biases):\n",
    "    layer_1_multiplication = tf.matmul(input_tensor, weights['h1'])\n",
    "    layer_1_addition = tf.add(layer_1_multiplication, biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1_addition)\n",
    "    \n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2_multiplication = tf.matmul(layer_1, weights['h2'])\n",
    "    layer_2_addition = tf.add(layer_2_multiplication, biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2_addition)\n",
    "    \n",
    "    # Output layer \n",
    "    out_layer_multiplication = tf.matmul(layer_2, weights['out'])\n",
    "    out_layer_addition = out_layer_multiplication + biases['out']\n",
    "    \n",
    "    return out_layer_addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "prediction = multilayer_perceptron(input_tensor, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=output_tensor))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# [NEW] Add ops to save and restore all the variables\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss= 1289.632795854\n",
      "Epoch: 0002 loss= 270.930032210\n",
      "Epoch: 0003 loss= 220.176122665\n",
      "Epoch: 0004 loss= 119.421511043\n",
      "Epoch: 0005 loss= 47.321779143\n",
      "Epoch: 0006 loss= 32.028330424\n",
      "Epoch: 0007 loss= 11.666836267\n",
      "Epoch: 0008 loss= 15.337957333\n",
      "Epoch: 0009 loss= 6.931441394\n",
      "Epoch: 0010 loss= 0.000000000\n",
      "Optimization Finished!\n",
      "Accuracy: 0.749153\n",
      "Model saved in path: /tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(newsgroups_train.data)/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x,batch_y = get_batch(newsgroups_train,i,batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            c,_ = sess.run([loss,optimizer], feed_dict={input_tensor: batch_x,output_tensor:batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(output_tensor, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    total_test_data = len(newsgroups_test.target)\n",
    "    batch_x_test,batch_y_test = get_batch(newsgroups_test,0,total_test_data)\n",
    "    print(\"Accuracy:\", accuracy.eval({input_tensor: batch_x_test, output_tensor: batch_y_test}))\n",
    "    \n",
    "    # [NEW] Save the variables to disk\n",
    "    save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a text to make a prediction\n",
    "text_for_prediction = newsgroups_test.data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text From: neff@iaiowa.physics.uiowa.edu (John S. Neff)\n",
      "Subject: Re: Space spinn offs\n",
      "Nntp-Posting-Host: pluto.physics.uiowa.edu\n",
      "Organization: The University of Iowa\n",
      "Lines: 23\n",
      "\n",
      "In article <1rruis$9do@bigboote.WPI.EDU> wfbrown@wpi.WPI.EDU (William F Brown) writes:\n",
      ">From: wfbrown@wpi.WPI.EDU (William F Brown)\n",
      ">Subject: Re: Space spinn offs\n",
      ">Date: 30 Apr 1993 19:27:24 GMT\n",
      ">I just wanted to point out, that Teflon wasn't from the space program.\n",
      ">It was from the WWII nuclear weapons development program.  Pipes in the \n",
      ">system for fractioning and enriching uranium had to be lined with it.\n",
      ">\n",
      ">Uranium Hexafloride was the chemical they turned the pitchblend into for\n",
      ">enrichment.  It is massively corrosive.  Even to Stainless steels. Hence\n",
      ">the need for a very inert substaance to line the pipes with.  Teflon has\n",
      ">all its molecular sockets bound up already, so it is very unreactive.\n",
      ">\n",
      ">My 2 sense worth.\n",
      ">\n",
      ">Bill\n",
      ">\n",
      "\n",
      "The artifical pacemaker was invented in 1958 by Wilson Greatbatch an\n",
      "American biomedical engineer. The bill authorizing NASA was signed\n",
      "in October of 1958 so it is clear that NASA had nothing to do with\n",
      "the invention of the pacemaker.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('text',text_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text correct category: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Text correct category:\", newsgroups_test.target[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert text to vector so we can send it to our model\n",
    "vector_txt = text_to_vector(text)\n",
    "# Wrap vector like we do in get_batches()\n",
    "input_array = np.array([vector_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 119930)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "Predicted category: [0]\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    classification = sess.run(tf.argmax(prediction, 1), feed_dict={input_tensor: input_array})\n",
    "    print(\"Predicted category:\", classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "Predicted categories: [1 0 2 0 1 2 1 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Get 10 texts to make a prediction\n",
    "\n",
    "x_10_texts,y_10_correct_labels = get_batch(newsgroups_test,0,10)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    classification = sess.run(tf.argmax(prediction, 1), feed_dict={input_tensor: x_10_texts})\n",
    "    print(\"Predicted categories:\", classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct categories: [1 0 1 0 1 2 2 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Correct categories:\", np.argmax(y_10_correct_labels, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
